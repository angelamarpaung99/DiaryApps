{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Deep Learning for NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "S17mlniR-cS1",
        "GlwqMD4TtSfz",
        "Rpw2aH1dRUwq",
        "3hdUQZ92tc4D",
        "YDhJ8JcFvF4J",
        "3pfNJKstwvjR",
        "w0Asl9BBG0hE"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelamarpaung99/DiaryApps/blob/master/Deep_Learning_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jSXkbKEPhre",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ezOFUz4aZM",
        "colab_type": "text"
      },
      "source": [
        "Silakan download file di bawah ini terlebih dahulu. Ukuran sekitar 800MB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBv6dtbk4GJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "87782a1a-446f-4a8a-9bc5-f2d5c93292b7"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-09 06:21:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-09 06:22:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-09 06:22:00--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip          8%[>                   ]  70.92M  5.62MB/s    eta 2m 5s  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17mlniR-cS1",
        "colab_type": "text"
      },
      "source": [
        "## Training Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phAZwxNB4uI1",
        "colab_type": "text"
      },
      "source": [
        "Import library yang dibutuhkan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj0VpY6cPyQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, string \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from sklearn.manifold import TSNE\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFPZtru46dt",
        "colab_type": "text"
      },
      "source": [
        "Download stopwords yang disediakan oleh NLTK. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZrK4XiUSI2e",
        "colab_type": "code",
        "outputId": "5545f408-9e07-4277-b7bc-fbc861961b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfT0Sex_6CaF",
        "colab_type": "text"
      },
      "source": [
        "Saat ini tersedia stopwords untuk 23 bahasa (mungkin bertambah)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIPqLC2r5xct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(stopwords.fileids())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLIApmc1SXEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ45Tcn9-S54",
        "colab_type": "text"
      },
      "source": [
        "Download dataset bbc news."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRWkdBYfRSgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\"\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ1Ckrgg-jyA",
        "colab_type": "text"
      },
      "source": [
        "Cek jumlah artikel yang merupakan jumlah baris pada dataframe (`df`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh3XqD-ecUVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtXjTz65RW7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Obq0qUC-6VQ",
        "colab_type": "text"
      },
      "source": [
        "Bersihkan data, dan hilangkan stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSGExrl9RaEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    # Remove a sentence if it is only one word long\n",
        "    if len(text) > 2:\n",
        "        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "\n",
        "df_clean = df.copy()\n",
        "df_clean['text'] = pd.DataFrame(df.text.apply(lambda x: clean_text(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB3lm9UzSjr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1MmXBXi_Bxh",
        "colab_type": "text"
      },
      "source": [
        "Terapkan lematisasi menggunakan library spacy. Cek https://spacy.io/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OlmEOmiSuHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
        "\n",
        "def lemmatizer(text):        \n",
        "    sent = []\n",
        "    doc = nlp(text)\n",
        "    for word in doc:\n",
        "        sent.append(word.lemma_)\n",
        "    return \" \".join(sent)\n",
        "\n",
        "df_clean[\"text_lemmatize\"] =  df_clean.apply(lambda x: lemmatizer(x['text']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRxhNetqTK_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZXJkKZETTAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean[\"text\"][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5iJ3uGxTnEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean[\"text_lemmatize\"][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEQO2kVAAt6q",
        "colab_type": "text"
      },
      "source": [
        "Persiapkan list `sentences`, yang berisi list kata pada tiap kalimat.  \n",
        "Dari `['saya dan dia', 'kamu dan mereka']` menjadi `[['saya', 'dan', 'dia'], ['kamu', 'dan', 'mereka']`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqlBfFjYT1Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [row.split() for row in df_clean['text_lemmatize']]\n",
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p8fjO-mAj9M",
        "colab_type": "text"
      },
      "source": [
        "Top-10 kata dengan jumlah kemunculan tertinggi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UHyM5yUIMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEyE0_v0PgLE",
        "colab_type": "text"
      },
      "source": [
        "Buat objek dari constructor Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQLDiDnUVQ0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# min_count: minimum number of occurrences of a word in the corpus to be included in the model.\n",
        "# window: the maximum distance between the current and predicted word within a sentence.\n",
        "# size: the dimensionality of the feature vectors\n",
        "# workers: number of workers, \n",
        "w2v_model = Word2Vec(min_count=200,\n",
        "                     window=5,\n",
        "                     size=100,\n",
        "                     workers=4,\n",
        "                     sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzVDuQAsPoDE",
        "colab_type": "text"
      },
      "source": [
        "Bangun vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA0qKc6LVZLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this line of code to prepare the model vocabulary\n",
        "w2v_model.build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQbB94_PPwki",
        "colab_type": "text"
      },
      "source": [
        "Train word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p85SjhcqVh7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train word vectors\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3o1Z6zkVkyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explore the model\n",
        "w2v_model.wv.most_similar(positive=['economy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBlcresgYcuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model.wv.most_similar(positive=['tv'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKvb0UZbYor3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model.wv.similarity('device', 'gadget')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfkClHdZZYXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model['gadget']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdq1L_B-ZklF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model['gadget'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlwqMD4TtSfz",
        "colab_type": "text"
      },
      "source": [
        "## Text Classification with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqgEFXhdP_ms",
        "colab_type": "text"
      },
      "source": [
        "Kali ini kita akan coba menggunakan LSTM untuk melakukan klasifikasi teks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MclTSZFZwp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1ywrQw-Bp8h",
        "colab_type": "text"
      },
      "source": [
        "`vocab_size`: hanya menggunakan top-5000 vocab dari total 24582. Hal ini untuk mempercepat waktu training.  \n",
        "`embedding_dim`: dimensi vektor representasi setiap token.  \n",
        "`max_length`: maksimal jumlah token dalam satu artikel/sample data.  \n",
        "`trunc_type = post`: jika melebihi `max_length`, pemotongan dilakukan di bagian akhir.  \n",
        "`oov_tok`: pengganti token yang tidak ada dalam vocab.\n",
        "\n",
        "*Sebaiknya menyebutkan token atau kata?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ltMUkNic9mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "training_portion = .8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5aPPDfhDnYT",
        "colab_type": "text"
      },
      "source": [
        "Split data menjadi 80% training dan 20% validasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_2fvkAxdmYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = int(len(df_clean) * training_portion)\n",
        "train_set = df_clean[0: train_size]\n",
        "validation_set = df_clean[train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYRei0Uddva8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_set), len(validation_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb4R_MKaD14N",
        "colab_type": "text"
      },
      "source": [
        "Tokenisasi menggunakan library `keras`. Fit `tokenizer` **hanya pada training set**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9F0zUrUdxTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_set[\"text\"])\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIks7Z3weQFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(word_index.items())[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aX_M2vlEw2Q",
        "colab_type": "text"
      },
      "source": [
        "Encode sekuen token ke dalam sekuen id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpt6CnOkeUJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_set[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JF_kuJpe06O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_sequences[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc--4nzJFfV_",
        "colab_type": "text"
      },
      "source": [
        "Potong sekuens ids (jika melebihi `max_length`) atau tambahkan padding (jika kurang dari `max_length`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGAiKQ7Te3rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4jpJaQBfA9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('before pad_sequences: ',len(train_sequences[0]))\n",
        "print('after pad_sequences: ',len(train_padded[0]))\n",
        "\n",
        "print('before pad_sequences: ',len(train_sequences[1]))\n",
        "print('after pad_sequences: ',len(train_padded[1]))\n",
        "\n",
        "print('before pad_sequences: ',len(train_sequences[10]))\n",
        "print('after pad_sequences: ',len(train_padded[10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J83nd_TvQZwq",
        "colab_type": "text"
      },
      "source": [
        "Perbandingan data sebelum dan setelah padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGu-eYZFfIA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.asarray(train_sequences[10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBctiuR9GJqO",
        "colab_type": "text"
      },
      "source": [
        "Token padding adalah `0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LamBFOkfNFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_padded[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJA5YfdGOSX",
        "colab_type": "text"
      },
      "source": [
        "Lakukan hal yang sama untuk data validasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AymBefAMfPNb",
        "colab_type": "code",
        "outputId": "afd33aa4-b54d-4f97-c7fa-dc92fccc1518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "validation_sequences = tokenizer.texts_to_sequences(validation_set['text'])\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(len(validation_sequences))\n",
        "print(validation_padded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "445\n",
            "(445, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf4Wn2FlGWD8",
        "colab_type": "text"
      },
      "source": [
        "Ubah label ke dalam numerik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOTW7Pfjf2Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(df_clean['category'])\n",
        "\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_set['category']))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_set['category']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqchcdwEgQHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(training_label_seq[0])\n",
        "print(training_label_seq[1])\n",
        "print(training_label_seq[2])\n",
        "print(training_label_seq.shape)\n",
        "\n",
        "print(validation_label_seq[0])\n",
        "print(validation_label_seq[1])\n",
        "print(validation_label_seq[2])\n",
        "print(validation_label_seq.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpLNjsq6Qhf1",
        "colab_type": "text"
      },
      "source": [
        "Sebelum kita mulai klasifikasi, mari kita lihat perubahan yang terjadi dari pada training data dalam format teks. Kata yang tidak ada dalam vocabulary (OOV) sudah digantikan dengan token khusus, dan ditambahkan `pad_token` yang didecode sebagai `?`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4RUFl9kihGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_article(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "print(decode_article(train_padded[10]))\n",
        "print('---')\n",
        "print(train_set['text'][10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpw2aH1dRUwq",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py2iD_i0SkLu",
        "colab_type": "text"
      },
      "source": [
        "Pertama kita perlu `Embedding` layer untuk mengubah token id menjadi vektor. Saat ini, weights dari embedding layer akan ditrain bersamaan dengan proses training semua layer.  \n",
        "Lalu sebuah `LSTM` layer dengan **jumlah `units`** dalam contoh ini yaitu 64.  \n",
        "Terakhir kita tambahkan 2 buah Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gdfiuZjiohw",
        "colab_type": "code",
        "outputId": "026c6df8-3536-459c-9abc-e822571cf75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.LSTM(embedding_dim),\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 64)          320000    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               66000     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6)                 606       \n",
            "=================================================================\n",
            "Total params: 396,706\n",
            "Trainable params: 396,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_5VAhh3i9m4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnEXdjmeTO54",
        "colab_type": "text"
      },
      "source": [
        "Train model yang telah dibuat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScxHIobGjB3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1bXo93sTVEQ",
        "colab_type": "text"
      },
      "source": [
        "Plot accuracy dan error dari traning dan validation. Terlihat dalam 10 epochs, akurasi validasi di sekitar 50%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eiob55_jF5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"acc\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hdUQZ92tc4D",
        "colab_type": "text"
      },
      "source": [
        "## BiLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NavxSQ69TtM-",
        "colab_type": "text"
      },
      "source": [
        "Sekarang kita akan coba menggunakan Bidirectional LSTM. `tf.keras` sudah menyiapkan layer khusus yang langsung bisa digunakan, yaitu `tf.keras.layers.Bidirectional`. Terlihat bahwa jumlah parameter pada layer bidirectional tepat 2 kali jumlah parameter di LSTM biasa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YIyVTI5l1TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_bi = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model_bi.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBMocRcmp9HC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_bi.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model_bi.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZT12ZikqJmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_graphs(history, \"acc\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDhJ8JcFvF4J",
        "colab_type": "text"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9-4yOVaUWK9",
        "colab_type": "text"
      },
      "source": [
        "Salah satu alasan menggunakan Bidirectional adalah perlunya kemampuan melihat input di depan (tidak hanya input sebelumnya). Maka Bidirectional bukan satu-satunya alternatif. CNN juga bisa menjadi alternatif karena CNN akan melihat beberapa input sebelum dan sesudah berdasarkan size filter yang diberikan.  \n",
        "  \n",
        "Di bawah ini, kita coba menggunakan jumlah filter sebanyak 128 dengan ukuran kernel 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQR8NCrasf6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                             input_length=max_length),\n",
        "    # specify the number of convolutions that you want to learn, their size, and their activation function.\n",
        "    # words will be grouped into the size of the filter in this case 5\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model_cnn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RQtIMMFvL2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model_cnn.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okxgga5WVVhz",
        "colab_type": "text"
      },
      "source": [
        "Dari plot, kita bisa lihat bahwa hasilnya hampir sama dengan model BiLSTM bahkan terlihat sedikit lebih baik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrLBtfH-vacD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_graphs(history, \"acc\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pfNJKstwvjR",
        "colab_type": "text"
      },
      "source": [
        "## Glove+CNN+LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OloQ7-frVrAB",
        "colab_type": "text"
      },
      "source": [
        "Kali ini kita coba untuk menggabungkan beberapa komponen, yaitu menggunakan `pretrained embedding matrix`, bisa menggunakan word2vec, namun di sini kita coba menggunakan Glove. Info tentang Glove: https://nlp.stanford.edu/projects/glove/. Ada beberapa pilihan Glove, kita coba menggunakan 100 dimensi.\n",
        "\n",
        "Lalu kita tambahkan CNN dan LSTM setelahnya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcSHcYBOzOHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "embedding_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4HRAgALWQNt",
        "colab_type": "text"
      },
      "source": [
        "Bentuk `embeddings_matrix` yang hanya berisi vektor kata yang ada dalam kamus dataset yang kita gunakan saat ini (BBC train)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjd5K5yd0ADl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {};\n",
        "with open('glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs;\n",
        "\n",
        "\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fx_e5YrWp8J",
        "colab_type": "text"
      },
      "source": [
        "Karena kita sudah menggunakan pretrained embeddings matrix, maka kita bisa set agar tidak mengupdate/learn weights nya. Set `trainable=False`.  \n",
        "Dari summary model terlihat bahwa meskipun jumlah total parameter jauh lebih banyak dari model-model sebelumnya, namun yang ditrain jauh lebih sedikit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoOjhEA50Xq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_combi = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model_combi.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model_combi.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHIW1g-b03L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_combi.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model_combi.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG7gbVvjXUD6",
        "colab_type": "text"
      },
      "source": [
        "Salah satu perbedaan yang terlihat adalah, hasil akurasi validasi pada epoch 1 sudah >80%, dibanding model sebelumnya yang berkisar 50%. Hal ini merupakan efek penggunaan pretrained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKqtsRkX1H1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_graphs(history, \"acc\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Asl9BBG0hE",
        "colab_type": "text"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqnXgprxXxY6",
        "colab_type": "text"
      },
      "source": [
        "Terakhir, mari kita coba menggunakan pretrained contextual word embeddings. Transformers menyediakan banyak sekali pilihan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFNNk7BQHME4",
        "colab_type": "text"
      },
      "source": [
        "Install transformers dari huggingface https://huggingface.co/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-t4M8yK9z4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psxejKxaHVA_",
        "colab_type": "text"
      },
      "source": [
        "Import beberapa library yang dibutuhkan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP6XBDjE1WYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import transformers as tfm # pytorch transformers\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3YDI3q9IbGQ",
        "colab_type": "text"
      },
      "source": [
        "Ada banyak model transformer yang bisa digunakan, saat ini kita coba menggunakan distilBERT. Model lainnya dapat dilihat di https://huggingface.co/transformers/pretrained_models.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7G0Abvj9lWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = (tfm.DistilBertModel, tfm.DistilBertTokenizer, 'distilbert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK0QISwPIw0R",
        "colab_type": "text"
      },
      "source": [
        "Load tokenizer dan juga model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV9JkIOK-1FE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKBBveH7I0ps",
        "colab_type": "text"
      },
      "source": [
        "Tokenize train_set dan juga validation_set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTQi47we-CYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sequences = train_set['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKVCz6sO-pCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_sequences = validation_set['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiDbJML3_IQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u43BlzLF_yBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SY7No_2Jz-h",
        "colab_type": "text"
      },
      "source": [
        "Ubah `train_padded` dan `validation_padded` ke dalam bentuk tensor. Lalu jalankan model (forward pass). DistilBERT memiliki hidden unit sebanyak 768."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqewXSNB__Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ids = torch.tensor(np.array(train_padded)).to(torch.int64)\n",
        "with torch.no_grad():\n",
        "    train_last_hidden_states = model(train_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZCYzPZcF5Ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_ids = torch.tensor(np.array(validation_padded)).to(torch.int64)\n",
        "with torch.no_grad():\n",
        "    validation_last_hidden_states = model(validation_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whSJAgCSYJ48",
        "colab_type": "text"
      },
      "source": [
        "Shape dari `last_hidden_states` yaitu `[jumlah data, panjang sekuens, jumlah hidden units]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGpIyhLUAJCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_last_hidden_states[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmxhgm2kYZOB",
        "colab_type": "text"
      },
      "source": [
        "Untuk teks classification, kita bisa menggunakan hanya output posisi pertama dari 200 output yang dikeluarkan model. Karena terdapat self-attention di dalam model tersebut, maka setiap posisi output akan memiliki aliran informasi dari semua posisi input, tidak hanya dari posisi input yang bersesuaian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqukCU22C4gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = train_last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFXaueUNGotp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_features = validation_last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80uMx-PiD-bX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIIuKhDvETKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label = training_label_seq.squeeze()\n",
        "validation_label = validation_label_seq.squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1wRsY-bZD71",
        "colab_type": "text"
      },
      "source": [
        "Coba gunakan logistic regression sederhana untuk melakukan klasifikasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEmGR2zOD0CR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_clf = LogisticRegression(max_iter=500)\n",
        "lr_clf.fit(train_features, train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx5vgLZMZIiv",
        "colab_type": "text"
      },
      "source": [
        "Ternyata akurasi train dan validationnya cukup tinggi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2oxzLNWEvOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_clf.score(train_features, train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kPC7Lv2FgJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_clf.score(validation_features, validation_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCNmfyZAGsll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKagruArjKMn",
        "colab_type": "text"
      },
      "source": [
        "## PoS Tagging with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtvQdy0QjzdL",
        "colab_type": "text"
      },
      "source": [
        "Originally from https://nlpforhackers.io/lstm-pos-tagger-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uLL4og6jKrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('treebank')\n",
        " \n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfvbWcTJjRCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences, sentence_tags =[], [] \n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        " \n",
        "# Let's see how a sequence looks\n",
        " \n",
        "print(sentences[5])\n",
        "print(sentence_tags[5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x8crBYTjVCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_sentences, \n",
        " test_sentences, \n",
        " train_tags, \n",
        " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOiP1rtFjYTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # The special value used to padding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ4Yb45GjbNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmtPXt_njdmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)  # 271"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWuOJ62DjgQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLr1HFUJjkNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(word2index), 128),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(tag2index))),\n",
        "    tf.keras.layers.Activation('softmax')\n",
        "])\n",
        "model.summary() \n",
        " \n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vViobsXSjnEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iZ8GyXajppq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXwKGEH5jr3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_c4hqSonqIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}